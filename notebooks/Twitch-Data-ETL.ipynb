{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "KoilKhAwOvdj"
      ],
      "mount_file_id": "10v5TlOqC5h0IK7WHO8iLnOj0BAqK-at7",
      "authorship_tag": "ABX9TyMeXw7yo/ZeOJ9DIvg9vAR9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jzhangfob/igdb-games-data-pipeline/blob/feature%2Fadd-notebook/notebooks/Twitch-Data-ETL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialization"
      ],
      "metadata": {
        "id": "Eh-vZIdSUtiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Import necessary packages (install google-cloud-storage if not already)\n",
        "- Mount Google Drive\n",
        "- Define export path variable to write raw CSV files to\n",
        "- Define API endpoints\n",
        "- Create global header variable to store authentication information"
      ],
      "metadata": {
        "id": "486H3kSG5K41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use of this notebook requires downloading a Service Account Key from GCP:\n",
        "\n",
        "1. Create a service account and download a service account key to your local machine\n",
        "2. Upload the service account JSON file to a Google Drive directory\n",
        "3. Define Google Application Credentials using the file path from step #2 above"
      ],
      "metadata": {
        "id": "uliFRoAMFR62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install google-cloud-storage"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kvJAjtI25Kae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYDM1xKuAvcU"
      },
      "outputs": [],
      "source": [
        "# Import packages\n",
        "import requests\n",
        "import csv\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "\n",
        "from google.cloud import storage\n",
        "from io import StringIO"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount GDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Vm0jcYLR3xL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the google application credentials path after uploading the service account key to Google Drive\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/drive/MyDrive/Twitch Data Pipeline/igdb-pipeline-a3bbac471b4c.json\""
      ],
      "metadata": {
        "id": "s_UIOx2jWInk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test that you can access GCS buckets\n",
        "client = storage.Client()\n",
        "buckets = list(client.list_buckets())\n",
        "print(buckets)  # Verifies that you can access your storage buckets"
      ],
      "metadata": {
        "id": "PJDvPjzYWOve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If exporting to Google Drive, define the directory\n",
        "EXPORT_PATH = '/content/drive/MyDrive/Twitch Data Pipeline/Raw'"
      ],
      "metadata": {
        "id": "1k4mpl88A97q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# All endpoints of interest\n",
        "end_point_games = 'https://api.igdb.com/v4/games'\n",
        "end_point_platforms = 'https://api.igdb.com/v4/platforms'\n",
        "end_point_game_modes = 'https://api.igdb.com/v4/game_modes'\n",
        "end_point_game_engines = 'https://api.igdb.com/v4/game_engines'\n",
        "end_point_genres = 'https://api.igdb.com/v4/genres'\n",
        "end_point_external_games = 'https://api.igdb.com/v4/external_games'\n",
        "\n",
        "# Endpoint dictionary\n",
        "end_point_dict = {\n",
        "    'games': end_point_games,\n",
        "    'platforms': end_point_platforms,\n",
        "    'game_modes': end_point_game_modes,\n",
        "    'game_engines': end_point_game_engines,\n",
        "    'genres': end_point_genres,\n",
        "    'external_games': end_point_external_games\n",
        "}"
      ],
      "metadata": {
        "id": "VMUIigM4U2PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pass in headers to api call\n",
        "HEADERS = {\n",
        "    'Client-ID': \"yzlyxaef51zs7qmklracxzbzuusrcf\",\n",
        "    'Authorization': \"Bearer 3itkqiiepb0ml35r2bw1pajtcgncib\"\n",
        "    }"
      ],
      "metadata": {
        "id": "R9znOL4ZUrHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions\n",
        "\n",
        "1. make_api_call\n",
        "  - Retrieves the data from a specified endpoint\n",
        "2. upload_dataframe_to_gcs\n",
        "  - Writes the data from make_api_call into a GCS storage bucket\n",
        "3. write_csv_from_api (Optional)\n",
        "  - Writes the data as a CSV for testing purposes"
      ],
      "metadata": {
        "id": "npftsfTvUonJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to make API calls to various endpoints\n",
        "def make_api_call(end_point, limit, offset, fields, header):\n",
        "  \"\"\"\n",
        "  Makes a request to an API endpoint with specified parameters and retrieves data.\n",
        "\n",
        "  Parameters:\n",
        "  ----------\n",
        "  end_point : str\n",
        "      The URL of the API endpoint to send the request to.\n",
        "  limit : int\n",
        "      The maximum number of records to return in a single API call.\n",
        "  offset : int\n",
        "      The starting position in the dataset from which records will be retrieved.\n",
        "  fields : str\n",
        "      A comma-separated string specifying the fields to include in the response.\n",
        "  header : dict\n",
        "      The headers for the API request, typically containing authentication details\n",
        "      (e.g., Client ID and authorization token).\n",
        "\n",
        "  Returns:\n",
        "  -------\n",
        "  pandas.DataFrame\n",
        "      A DataFrame containing all the retrieved data from the specified API endpoint.\n",
        "  \"\"\"\n",
        "\n",
        "  # Start logging message\n",
        "  print(f\"Beginning API call for endpoint: {end_point}\\n--------------\")\n",
        "  # Initialize sentinel value and an empty dataframe to store all API data\n",
        "  results_len = 1\n",
        "  all_df = pd.DataFrame()\n",
        "\n",
        "  # Continue the loop until all data from the API has been extracted\n",
        "  while results_len != 0:\n",
        "\n",
        "    try:\n",
        "      # if end_point == 'https://api.igdb.com/v4/game_engines':\n",
        "      #   params = {\n",
        "      #       'fields':\"*; exclude description;\",\n",
        "      #       'limit':limit,\n",
        "      #       'offset':offset\n",
        "      #       }\n",
        "      # # Set the parameters\n",
        "      # else:\n",
        "      #   params = {'fields':fields, 'limit':limit, 'offset':offset}\n",
        "      params = {'fields':fields, 'limit':limit, 'offset':offset}\n",
        "\n",
        "      # Make the API call and validate response status\n",
        "      r = requests.get(end_point, headers = header, params = params)\n",
        "      if r.status_code != 200:\n",
        "        raise Exception(f\"API call failed with status code {r.status_code}: {r.text}\")\n",
        "\n",
        "      # Print confirmation\n",
        "      print(f\"Getting the results for {r.url}\")\n",
        "\n",
        "      # Parse JSON response and check its structure\n",
        "      results = r.json()\n",
        "      if not isinstance(results,list):\n",
        "        raise ValueError(f\"Unexpected response format for {r.url}. Expected a list of records.\")\n",
        "\n",
        "      # Update results length\n",
        "      results_len = len(results)\n",
        "      print(f\"Received {results_len} records from {r.url}\")\n",
        "\n",
        "      # Add results to the dataframe (all_df)\n",
        "      if results_len > 0:\n",
        "        batch_results_df = pd.DataFrame(results)\n",
        "        all_df = pd.concat([all_df, batch_results_df], ignore_index=True)\n",
        "\n",
        "      # Increment offset for the next batch\n",
        "      offset += limit\n",
        "\n",
        "      # Maximum of 4 api calls per second\n",
        "      time.sleep(.25)\n",
        "\n",
        "    # Stop the loop on network failure\n",
        "    except requests.exceptions.RequestException as e:\n",
        "      print(f\"Network-related error occurred: {e}\")\n",
        "      break\n",
        "    # Stop the loop on unexpected errors\n",
        "    except Exception as e:\n",
        "      print(f\"An error occurred: {e}\")\n",
        "      break\n",
        "\n",
        "  # Print confirmation message\n",
        "  print(f'Finished retrieving data from {end_point}')\n",
        "  print(f'Total records retrieved: {all_df.shape[0]}')\n",
        "  # End logging message\n",
        "  print(f\"Finished API call for endpoint: {end_point}\\n--------------\")\n",
        "\n",
        "  return all_df\n"
      ],
      "metadata": {
        "id": "-7CDQSYftt47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_csv_from_api(api_data, path, data_type):\n",
        "  # Create the file path if it does not exist\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "  # Write the df as a csv\n",
        "  final_path = os.path.join(EXPORT_PATH, f'{data_type}.csv')\n",
        "  api_data.to_csv(final_path, index=False)\n",
        "\n",
        "  # Print message\n",
        "  print(f\"Wrote {data_type} data to {final_path}\")"
      ],
      "metadata": {
        "id": "y_Mkn5Orv4nD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_dataframe_to_gcs(df, bucket_name, destination_blob_name):\n",
        "    \"\"\"\n",
        "    Writes a pandas DataFrame to Google Cloud Storage as a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    df (pandas.DataFrame): The DataFrame to upload.\n",
        "    bucket_name (str): The name of the GCS bucket.\n",
        "    destination_blob_name (str): The destination path within the bucket.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Convert DataFrame to CSV\n",
        "    csv_buffer = StringIO()\n",
        "    df.to_csv(csv_buffer, index=False)\n",
        "    # Reset buffer position to the beginning\n",
        "    csv_buffer.seek(0)\n",
        "\n",
        "    # Initialize GCS client\n",
        "    client = storage.Client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "    # Upload the file\n",
        "    blob.upload_from_string(csv_buffer.getvalue(), content_type='text/csv')\n",
        "    print(f\"Data uploaded to {bucket_name}/{destination_blob_name}\")"
      ],
      "metadata": {
        "id": "vh2wuz_W8KOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main function\n",
        "\n",
        "Loops through the end point dictionary to retrieve data and writes it to the specified path"
      ],
      "metadata": {
        "id": "5dMoOtuG8QcH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Store dataframes from API calls separately\n",
        "all_api_df = []\n",
        "\n",
        "# Loop through the end point dict to make API calls\n",
        "for data_type in end_point_dict:\n",
        "  # Retrieve data\n",
        "  data = make_api_call(\n",
        "      end_point=end_point_dict[data_type],\n",
        "      limit=500,\n",
        "      offset=0,\n",
        "      fields=\"*\",\n",
        "      header=HEADERS\n",
        "  )\n",
        "\n",
        "  all_api_df.append(data)\n",
        "  # print(f\"Columns from {data_type}: {data.columns}\\n\")\n",
        "\n",
        "  # Write data to GCS bucket\n",
        "  upload_dataframe_to_gcs(\n",
        "      df=data,\n",
        "      bucket_name=\"igdb_raw_data_bucket\",\n",
        "      destination_blob_name=data_type\n",
        "  )\n"
      ],
      "metadata": {
        "id": "RMK6ctgM1jrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for df in all_api_df:\n",
        "  print(df.info())"
      ],
      "metadata": {
        "id": "zEd_xBabf1d2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xjO8LybWZuZD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}